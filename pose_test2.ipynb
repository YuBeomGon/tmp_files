{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b72d7cf7-697f-4813-b031-b91a8c679455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import PIL\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7118fbf5-c917-4faf-9914-f4713e898a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  0,  1],\n",
       "        [ 2,  3,  1,  2],\n",
       "        [ 4,  5,  2,  3],\n",
       "        [ 6,  7,  3,  4],\n",
       "        [ 8,  9,  4,  5],\n",
       "        [10, 11,  5,  6],\n",
       "        [12, 13,  2,  4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import trt_pose.coco\n",
    "import timm\n",
    "import time\n",
    "\n",
    "with open('human_pose_2.json', 'r') as f:\n",
    "    human_pose = json.load(f)\n",
    "\n",
    "topology = trt_pose.coco.coco_category_to_topology(human_pose)\n",
    "topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453dca61-49b2-4478-b31f-242f237a0d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adffb6d8-fa06-43ca-aec0-e55687dcca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trt_pose.models\n",
    "\n",
    "num_parts = len(human_pose['keypoints'])\n",
    "num_links = len(human_pose['skeleton'])\n",
    "\n",
    "model = trt_pose.models.efficientnet_b3_baseline_att(num_parts, 2 * num_links).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b61386-1b2b-4bb6-8f43-2e68594f624a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0642,  0.0589, -0.0545],\n",
       "        [ 0.1169, -0.2110, -0.4134],\n",
       "        [ 0.1067,  0.0160, -0.1370]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].efficientnet.conv_stem.weight[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55495a2b-c77a-4c89-a79c-8c37673fb4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "MODEL_WEIGHTS = 'epoch_200.pth'\n",
    "state_dict = torch.load(MODEL_WEIGHTS)\n",
    "\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "    \n",
    "model.load_state_dict(new_state_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc963acb-b1da-413a-ae3f-668951ac0ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4986,  0.1909, -0.2082],\n",
       "        [ 0.4296, -0.1981, -2.0589],\n",
       "        [ 0.2448,  0.4187, -0.4783]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].efficientnet.conv_stem.weight[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91f2f0cf-7870-41d7-b6af-362effb4bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c1e3f56-f398-4838-97dd-2547a3f56e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch2trt\n",
    "# WIDTH = SIZE\n",
    "# HEIGHT = SIZE\n",
    "# data = torch.zeros((1, 3, HEIGHT, WIDTH)).cuda()\n",
    "\n",
    "# model_trt = torch2trt.torch2trt(model, [data], fp16_mode=False, max_workspace_size=1<<25)\n",
    "# model_trt = torch2trt.torch2trt(model, [data], fp16_mode=True, max_workspace_size=3000)\n",
    "\n",
    "OPTIMIZED_MODEL = 'epoch_200_fp16_trt.pth'\n",
    "# torch.save(model_trt.state_dict(), OPTIMIZED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f88e8c-69f0-4a45-ab6f-0f705dcc5847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch2trt import TRTModule\n",
    "\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c52a190-331d-4934-ba2e-e5f3e13017bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'shelfNum': 8, 'annotation': [[257, 74, 1, 317, 66, 1, 376, 56, 1, 447, 51, 1, 511, 44, 1, 584, 34, 1], [287, 130, 1, 344, 124, 1, 404, 116, 1, 465, 111, 1, 528, 107, 1, 595, 104, 1], [341, 189, 1, 418, 189, 1, 504, 182, 1, 589, 184, 1], [372, 245, 1, 445, 245, 1, 518, 243, 1, 595, 242, 1], [413, 289, 1, 502, 291, 1, 593, 290, 1]]}, {'shelfNum': 9, 'annotation': [[702, 42, 1, 772, 42, 1, 845, 43, 1, 911, 49, 1, 965, 52, 1, 1030, 64, 1], [695, 110, 1, 757, 112, 1, 815, 116, 1, 878, 121, 1, 938, 128, 1, 996, 133, 1], [703, 181, 1, 789, 187, 1, 877, 192, 1, 944, 192, 1], [699, 242, 1, 772, 240, 1, 844, 243, 1, 915, 242, 1], [703, 289, 1, 790, 289, 1, 878, 290, 1]]}, {'shelfNum': 10, 'annotation': [[1112, 82, 1, 1158, 87, 1, 1202, 93, 1, 1237, 104, 1, 1270, 109, 1, 0, 0, 0], [1070, 143, 1, 1111, 144, 1, 1150, 150, 1, 1196, 158, 1, 1232, 165, 1, 1264, 173, 1], [1031, 195, 1, 1097, 205, 1, 1152, 209, 1, 1203, 218, 1], [1016, 248, 1, 1055, 251, 1, 1116, 252, 1, 1167, 260, 1], [984, 292, 1, 1051, 295, 1, 1115, 295, 1]]}]\n"
     ]
    }
   ],
   "source": [
    "# from trt_pose.draw_objects import DrawObjects\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "# from draw import DrawObjects, SaveImage\n",
    "from utils import GetKeysBoxes, visualize, DrawObjects, get_matching, visualize_goods, personId_matching\n",
    "import msg_utils\n",
    "\n",
    "parse_objects = ParseObjects(topology)\n",
    "draw_objects = DrawObjects(topology)\n",
    "# save_image = SaveImage(topology)\n",
    "get_keysboxes = GetKeysBoxes(topology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebe8b3ba-58ad-4178-8334-a09e190b679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sort import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bc8a918-5bc3-4c82-a629-71a791107cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 200, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "306abda7-0919-4caf-ad6b-d3a455b002cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mot_tracker = Sort(max_age=5, iou_threshold=0.15) #create instance of the SORT tracker\n",
    "mot_tracker = Sort() #create instance of the SORT tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "908ad97e-637c-4bed-ae6f-b6b7b0089f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n",
      "[]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "device = torch.device('cuda')\n",
    "\n",
    "\n",
    "# cap = cv2.VideoCapture('./video/video_sample_34_1.mp4')\n",
    "# cap = cv2.VideoCapture('./video/2person_35.h264')\n",
    "cap = cv2.VideoCapture('./video/out_3.mp4')\n",
    "\n",
    "pTime = 0\n",
    "img_array = []\n",
    "count = 0\n",
    "\n",
    "persons = {}\n",
    "\n",
    "# need to keep person's goods\n",
    "person_goods = {}\n",
    "\n",
    "while True :\n",
    "    success, img = cap.read()\n",
    "\n",
    "    if success == False :\n",
    "        break\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    resized_image = cv2.resize(img, (SIZE,SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    image = resized_image / 255.\n",
    "    image = (image - mean[None, None, :])/(std[None, None, :])\n",
    "    image = torch.tensor(image, dtype=torch.float32).permute(2,0,1)\n",
    "    image = image.to(device)    \n",
    "    \n",
    "    cmap, paf = model_trt(image[None, ...])\n",
    "    # cmap, paf = model(image[None, ...])\n",
    "    cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "    counts, objects, peaks = parse_objects(cmap, paf) \n",
    "    boxes, keys = get_keysboxes(resized_image, counts, objects, peaks)\n",
    "    \n",
    "    # goods_point = get_goods_point()\n",
    "    # goods = barcode_to_goods(barcode)\n",
    "    # msg = consumer.poll(0.001)\n",
    "    kmsg = msg_utils.kafka_pog_msg()\n",
    "    \n",
    "    goods_point = None\n",
    "    goods = None\n",
    "    \n",
    "    if kmsg != None :\n",
    "        shelf_num, shelf_row, col, barcode, qty = kmsg\n",
    "        if shelf_num == 5:\n",
    "            shelf_num = 10\n",
    "        if shelf_num == 6:\n",
    "            shelf_num = 9\n",
    "        if shelf_num == 7:\n",
    "            shelf_num = 8    \n",
    "            \n",
    "        if shelf_num < 5 :\n",
    "            cam_num = 0\n",
    "        else :\n",
    "            cam_num = 1\n",
    "        print(msg_utils.pd_coord(0, 2, 2, 3))\n",
    "        goods_point = msg_utils.pd_coord(cam_num, shelf_num, shelf_row, col)\n",
    "        print(goods_point)\n",
    "        # goods = barcode_to_goods[barcode]\n",
    "\n",
    "    if len(boxes) > 0  :\n",
    "        # print('*****************************************************************************')\n",
    "        boxes = np.array(boxes)\n",
    "        # print(boxes)\n",
    "        prob = np.ones((boxes.shape[0], 1)).astype(np.float32)\n",
    "        boxes1 = np.c_[boxes, prob]\n",
    "        trackers = mot_tracker.update(boxes1).astype(np.uint)\n",
    "        # print(trackers)\n",
    "        \n",
    "        m = iou_batch(boxes, trackers[:,:4])\n",
    "        # print(m)\n",
    "        index = np.argmax(m, axis=0)\n",
    "        m_keys = [keys[i] for i in index]\n",
    "        \n",
    "        persons = dict([(key, m_keys[i]) for i, key in enumerate(trackers[:,4])])\n",
    "        # print(person)\n",
    "        # goods_point = get_goods_point()\n",
    "        # goods = barcode_to_goods(barcode)\n",
    "        \n",
    "        if goods_point != None and goods != None and len(trackers) > 0 :\n",
    "            # print(persons)\n",
    "            person_id, nearest_body = get_matching(persons, goods_point)\n",
    "            # print(person_id)\n",
    "            # print(trackers[:,4])\n",
    "            \n",
    "            # print(person_id)\n",
    "            new, remove, add = personId_matching(trackers[:,4], list(person_goods.keys()))\n",
    "            \n",
    "            if len(new) > 0 :\n",
    "                for n in new :\n",
    "                    if person_id == n :\n",
    "                        person_goods[person_id] = {'point': nearest_body, \"goods\": [goods] }\n",
    "                    else :\n",
    "                        person_goods[person_id] = {}\n",
    "                    \n",
    "            if len(add) > 0 :\n",
    "                for a in add :\n",
    "                    if person_id == a :\n",
    "                        if 'goods' in person_goods[a].keys() :\n",
    "                            person_goods[person_id]['goods'].append(goods)\n",
    "                            person_goods[person_id]['point'] = nearest_body\n",
    "                        else :\n",
    "                            person_goods[person_id] = {'point': nearest_body, \"goods\": [goods] }\n",
    "                    # else :\n",
    "                    #     pass\n",
    "                        \n",
    "            if len(remove) > 0:\n",
    "                for rm in remove :\n",
    "                    del person_goods[rm]\n",
    "\n",
    "            # if person_id in person_goods.keys() :\n",
    "            #     person_goods[person_id]['point'] = nearest_body\n",
    "            #     person_goods[person_id]['goods'].append(goods)\n",
    "            # else :\n",
    "            #     person_goods[person_id] = {'point': nearest_body, \"goods\": [goods] }\n",
    "            \n",
    "            # print(trackers[:,4])\n",
    "            # print(person_goods)\n",
    "             \n",
    "        # resized_image = visualize(resized_image, boxes1[:,:4], boxes1[:,4])\n",
    "        # resized_image = visualize(resized_image, trackers[:,:4], trackers[:,4])\n",
    "    # resized_image = draw_objects(resized_image, counts, objects, peaks, 'test')\n",
    "    resized_image = visualize_goods(resized_image, person_goods)\n",
    "        \n",
    "    img_array.append(resized_image)\n",
    "\n",
    "out = cv2.VideoWriter('./video/output.mp4', cv2.VideoWriter_fourcc(*'DIVX'), 4, (SIZE, SIZE))\n",
    "for i in range(len(img_array)) :\n",
    "    out.write(img_array[i])\n",
    "    \n",
    "out.release()    \n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277bb35f-66fd-44b1-8b05-9fa3136ee3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40778e37-80e0-448e-af3a-c79daeac7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ab079d8-ebde-44d6-996d-a8ea2c4744c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(person_goods.keys())\n",
    "a = [1, 2, 3]\n",
    "a.remove(1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "736a9e76-7698-451c-a770-f0195d485d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = {24.0: [[119, 147], [106, 165], [119, 194]], 21.0: [[235, 101], [249, 99], [213, 105]], 17.0: [[261, 251], [285, 231], [270, 226]]}\n",
    "goods_point = (125, 210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab407c41-c86a-4e91-9dd0-99fa966e5dac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'point' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m     nearest_p \u001b[38;5;241m=\u001b[39m persons[ids[index_i]][index_y]\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ids[index_i], nearest_p\n\u001b[0;32m---> 12\u001b[0m get_matching(persons, \u001b[43mpoint\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'point' is not defined"
     ]
    }
   ],
   "source": [
    "def get_matching(persons, goods_point) :  \n",
    "    ids = list(persons.keys())\n",
    "    diff = [ [ np.sqrt((goods_point[0] - p[0])**2 + (goods_point[1] - p[1])**2)  for p in points ]for points in persons.values()]\n",
    "    \n",
    "    index = [ min(d) for d in diff]\n",
    "    index_i = index.index(min(index))\n",
    "    index_y = diff[index_i].index(min(diff[index_i]))    \n",
    "    nearest_p = persons[ids[index_i]][index_y]\n",
    "\n",
    "    return ids[index_i], nearest_p\n",
    "    \n",
    "get_matching(persons, point)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9716a742-058c-4fc8-ae3a-7e9526c3e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_test = [[201.,  98., 249., 129.],\n",
    " [107., 133., 134., 194.],\n",
    " [  2., 190.,  62., 226.],\n",
    " [247., 199., 287., 250.]]\n",
    "# [21. 17.]\n",
    "bb_gt = [[201,  96, 248, 130],\n",
    " [247, 200, 285, 252],\n",
    "        [  2., 190.,  62., 226.]]\n",
    "bb_test = np.array(bb_test)\n",
    "bb_gt = np.array(bb_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d7c6cf-8f78-440a-bb4c-86ebce8b72fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = iou_batch(bb_test, bb_gt)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3806a0-08ea-436d-9f6c-037b83badc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(m, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3504b5-588c-4919-b79b-1f0697a2e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a230f983-cd37-4a6f-b6d0-b6db27e65734",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac688f6-8d02-4d80-905a-4d8287b0f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects[0,:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca752f54-0bdc-4185-8ffb-8af774bda878",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155762d-326c-4354-998f-576f33bf3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ebfd7-5184-4d4b-b6f9-5d94109e0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls '../../../images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d2643-2884-476b-bc33-f98ef04a253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../../../images/' + '694.jpg')\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b144a3-4cb9-4cfb-aa55-3d0a13f86fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cmap[0,0,:,:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1dfe87-d7eb-4b91-89d2-6ac7e0f99487",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../../../images/' + '694.jpg')\n",
    "SIZE = 320\n",
    "resized_image = cv2.resize(img, (SIZE,SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "image = resized_image / 255.\n",
    "image = (image - mean[None, None, :])/(std[None, None, :])\n",
    "\n",
    "image = torch.tensor(image, dtype=torch.float32).permute(2,0,1)\n",
    "image = image.to(device)   \n",
    "\n",
    "cmap, paf = model(image[None, ...])\n",
    "cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "counts, objects, peaks = parse_objects(cmap, paf)  \n",
    "image = draw_objects(resized_image, counts, objects, peaks)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4da606-ede8-4750-b6fb-c9c1a0a90acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(paf[0,0,:,:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8791bd9-d687-42d4-a743-8771c4ae3d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cmap[0,0,:,:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9410dd4e-666a-4a2d-b618-5db3f6320fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cmap[0,6,:,:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fb0546-4d10-4429-8261-8ca2f8001579",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(paf[0,0,:,:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929357a5-22e5-4795-83a0-6aea1e5612e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(paf[0,1,:,:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad40939-9711-42d0-b9fd-e6256f0d48dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(paf[0,10,:,:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b0fa8-7580-4dde-a204-e94ad91482e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(paf[0,11,:,:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c77491-dca4-40de-b2a4-eb44ecbeba1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378fc0a9-5696-430e-b723-a0d42702ce7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0725d1-b446-4c44-8b89-b463ed867107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94289a97-57a7-4f2e-96b8-666c86c1b54d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f24127-8dba-4ae5-92f2-9ccff5e0d987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcffc44-5924-48ae-8d98-df1b01ed5827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae362e18-c352-4bbe-966a-fbd4e9007bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae68952-258b-446a-a3a0-168a27fd986b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c2be52-ee69-44d5-9c4b-e63119412da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ff4565-6df4-4f63-b1cb-43d2643857ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e66697-1d37-41e7-a4d3-45949e1d35ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e9348c-0ee5-4bd8-a62a-d0477528c71e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e80bc5-317d-4f0d-a72e-2fc40ca7da02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b3b3ae-8c9d-4eb3-978a-cf2bc2b77a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80bbe7-dde5-43cf-9fe4-83920a081d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d3b00b-81ef-4a1b-a7f4-19ec4812a2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf97d2-e302-4eeb-978d-4ddef3603fea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
